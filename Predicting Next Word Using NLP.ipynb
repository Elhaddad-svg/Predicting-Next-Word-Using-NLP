{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSB3DRTiAUoA"
      },
      "outputs": [],
      "source": [
        "!pip install pad_sequences\n",
        "!pip install keras\n",
        "!pip install tensorflow\n",
        "!pip install opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_preprocessing"
      ],
      "metadata": {
        "id": "XABEZfEyvFQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !unrar x /content/enronsent.tar.gz\n",
        "!unzip \"/content/enronsentv.zip\" -d \"/content\""
      ],
      "metadata": {
        "id": "VatqqPXeB1yq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "210f319a-6590-4b1f-cec1-c92ee6506fac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/enronsentv.zip\n",
            "   creating: /content/enronsentv/\n",
            "  inflating: /content/enronsentv/enronsent00  \n",
            "  inflating: /content/enronsentv/enronsent01  \n",
            "  inflating: /content/enronsentv/enronsent02  \n",
            "  inflating: /content/enronsentv/enronsent03  \n",
            "  inflating: /content/enronsentv/enronsent04  \n",
            "  inflating: /content/enronsentv/enronsent05  \n",
            "  inflating: /content/enronsentv/enronsent06  \n",
            "  inflating: /content/enronsentv/enronsent07  \n",
            "  inflating: /content/enronsentv/enronsent08  \n",
            "  inflating: /content/enronsentv/enronsent09  \n",
            "  inflating: /content/enronsentv/enronsent10  \n",
            "  inflating: /content/enronsentv/enronsent11  \n",
            "  inflating: /content/enronsentv/enronsent12  \n",
            "  inflating: /content/enronsentv/enronsent13  \n",
            "  inflating: /content/enronsentv/enronsent14  \n",
            "  inflating: /content/enronsentv/enronsent15  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import liberaris\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.layers import Dense\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.callbacks import *\n",
        "from keras.utils import *\n",
        "import os\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "import nltk\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import PlaintextCorpusReader\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# Download the necessary data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eafJvY8CIf0",
        "outputId": "8ff35ed3-3d15-43d9-996e-8bf6bd2eac5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to the Enron Sent Corpus Dataset\n",
        "corpus_root = '/content/enronsentv'\n",
        "\n",
        "# Set the file ids of the data files\n",
        "file_ids_train = ['enronsent00', 'enronsent01','enronsent02', 'enronsent03', 'enronsent04',\n",
        "            'enronsent05','enronsent06','enronsent07','enronsent08','enronsent09','enronsent10']\n",
        "file_ids_test = ['enronsent11','enronsent12','enronsent13','enronsent14', 'enronsent15']\n",
        "\n",
        "# Initialize an empty list to store the tokens\n",
        "tokens_train = []\n",
        "tokens_test = []\n",
        "\n",
        "# Load and process each data file\n",
        "for file_id in file_ids_train:\n",
        "    # Load the data\n",
        "    corpus = PlaintextCorpusReader(corpus_root, file_id)\n",
        "    # Tokenize the data\n",
        "    tokens_train.extend(corpus.sents())\n",
        "\n",
        "# Load and process each data file\n",
        "for file_id in file_ids_test:\n",
        "    # Load the data\n",
        "    corpus = PlaintextCorpusReader(corpus_root, file_id)\n",
        "    # Tokenize the data\n",
        "    tokens_test.extend(corpus.sents())"
      ],
      "metadata": {
        "id": "qIyoy2BIYEjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove punctuation, numbers, and lemmatize the tokens\n",
        "# we will not remove stopwords as we also need to predict it\n",
        "punctuation = set(string.punctuation)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Preprocess the training data\n",
        "tokens_train = [[lemmatizer.lemmatize(token) for token in sent if token not in punctuation and not token.isdigit()] for sent in tokens_train]\n",
        "\n",
        "# Preprocess the testing data\n",
        "tokens_test = [[lemmatizer.lemmatize(token) for token in sent if token not in punctuation and not token.isdigit()] for sent in tokens_test]"
      ],
      "metadata": {
        "id": "UEQUh-2lcvZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the number of time steps\n",
        "time_steps = 5\n",
        "x_train = []\n",
        "y_train = []\n",
        "x_test = []\n",
        "y_test = []\n",
        "\n",
        "# Split each sample into fixed time steps for training data\n",
        "for token in tokens_train:\n",
        "    # Create input and output sequences\n",
        "    for i in range(len(token) - time_steps):\n",
        "        # Extract the input sequence of length time_steps\n",
        "        x_train.append(token[i:i+time_steps])\n",
        "        # Extract the next word as the output sequence\n",
        "        y_train.append(token[i+time_steps])\n",
        "\n",
        "# Split each sample into fixed time steps for testing data\n",
        "for token in tokens_test:\n",
        "    # Create input and output sequences\n",
        "    for i in range(len(token) - time_steps):\n",
        "        # Extract the input sequence of length time_steps\n",
        "        x_test.append(token[i:i+time_steps])\n",
        "        # Extract the next word as the output sequence\n",
        "        y_test.append(token[i+time_steps])"
      ],
      "metadata": {
        "id": "Xf8ziU6EfBEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train our word2vec model\n",
        "# Combine training and testing tokens into a single list\n",
        "word_train = tokens_train + tokens_test\n",
        "\n",
        "# Train Word2Vec model on the combined tokens\n",
        "word_model = Word2Vec(word_train, min_count=1, vector_size=25)\n",
        "\n",
        "# Save the trained Word2Vec model\n",
        "word_model.save(\"word2vec_model.bin\")"
      ],
      "metadata": {
        "id": "-3xI732yfA3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the vocabulary size and embedding dimension from the pre-trained model\n",
        "vocab_size = len(word_model.wv.index_to_key)\n",
        "embedding_dim = word_model.wv.vector_size\n",
        "\n",
        "# create a weight matrix for the embedding layer\n",
        "embedding_matrix = word_model.wv.vectors\n",
        "\n",
        "# print the info\n",
        "print(f\"\"\"vocab size : {vocab_size}\n",
        "embedding_dim: {embedding_dim}\n",
        "embedding_matrix shape: {embedding_matrix.shape}\"\"\")"
      ],
      "metadata": {
        "id": "0PUH81isfAwT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d63a0801-bfdf-4ba3-81e2-a2c43ba109b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size : 87090\n",
            "embedding_dim: 25\n",
            "embedding_matrix shape: (87090, 25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a tokenizer and fit it on our text data\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "\n",
        "# encode the text data as sequences of integers\n",
        "x_train_vectors = tokenizer.texts_to_sequences(x_train)\n",
        "x_test_vectors = tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "# pad the sequences to have the same length\n",
        "maxlen = max(len(seq) for seq in x_train)\n",
        "x_train_vectors = pad_sequences(x_train_vectors, maxlen=time_steps)\n",
        "x_test_vectors = pad_sequences(x_test_vectors, maxlen=time_steps)"
      ],
      "metadata": {
        "id": "EpNeHCpjY8gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a tokenizer and fit it on our text data\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(y_train)\n",
        "\n",
        "# encode the text data as sequences of integers\n",
        "y_train_vectors = tokenizer.texts_to_sequences(y_train)\n",
        "y_test_vectors = tokenizer.texts_to_sequences(y_test)\n",
        "\n",
        "# pad the sequences to have the same length\n",
        "maxlen = max(len(seq) for seq in x_train)\n",
        "y_train_vectors = pad_sequences(y_train_vectors, maxlen=1)\n",
        "y_test_vectors = pad_sequences(y_test_vectors, maxlen=1)"
      ],
      "metadata": {
        "id": "w2oJyx2Io_4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the data to numpy arrays\n",
        "x_train_vectors = np.array(x_train_vectors)\n",
        "y_train_vectors = np.array(y_train_vectors)\n",
        "x_test_vectors = np.array(x_test_vectors)\n",
        "y_test_vectors = np.array(y_test_vectors)"
      ],
      "metadata": {
        "id": "z7WG6ilzK-kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_vectors.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da1c4cac-6f34-4537-c530-e3083e92638b",
        "id": "fybkVVkGxpqa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2663198, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create the LSTM model\n",
        "model = Sequential()\n",
        "\n",
        "# Add an embedding layer with pre-trained word embeddings\n",
        "model.add(Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=time_steps, trainable=False))\n",
        "\n",
        "# Add batch normalization layer\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Add LSTM layer\n",
        "model.add(LSTM(128))\n",
        "\n",
        "# Add a dense layer with activation function 'relu'\n",
        "model.add(Dense(1, activation='relu'))\n",
        "\n",
        "# Compile the model with mean squared error loss and Adam optimizer\n",
        "model.compile(loss='mse', optimizer='adam')"
      ],
      "metadata": {
        "id": "LQgdV0EdoX7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the LSTM model on your data\n",
        "model.fit(x_train_vectors,y_train_vectors, validation_data=(x_test_vectors, y_test_vectors), epochs=10, verbose=1,batch_size=64)"
      ],
      "metadata": {
        "id": "z5zk6lFdfAYm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7917dd1-2f0b-401a-885d-d501e2a889ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "41613/41613 [==============================] - 243s 6ms/step - loss: 17607494.0000 - val_loss: 15952373.0000\n",
            "Epoch 2/10\n",
            "41613/41613 [==============================] - 230s 6ms/step - loss: 16973204.0000 - val_loss: 15820531.0000\n",
            "Epoch 3/10\n",
            "41613/41613 [==============================] - 232s 6ms/step - loss: 16767962.0000 - val_loss: 15790979.0000\n",
            "Epoch 4/10\n",
            "41613/41613 [==============================] - 218s 5ms/step - loss: 16635647.0000 - val_loss: 15793943.0000\n",
            "Epoch 5/10\n",
            "41613/41613 [==============================] - 228s 5ms/step - loss: 16541855.0000 - val_loss: 15814003.0000\n",
            "Epoch 6/10\n",
            "41613/41613 [==============================] - 227s 5ms/step - loss: 16466976.0000 - val_loss: 15782689.0000\n",
            "Epoch 7/10\n",
            "41613/41613 [==============================] - 223s 5ms/step - loss: 16408297.0000 - val_loss: 15753175.0000\n",
            "Epoch 8/10\n",
            "41613/41613 [==============================] - 217s 5ms/step - loss: 16354979.0000 - val_loss: 15782681.0000\n",
            "Epoch 9/10\n",
            "41613/41613 [==============================] - 216s 5ms/step - loss: 16303956.0000 - val_loss: 15812660.0000\n",
            "Epoch 10/10\n",
            "41613/41613 [==============================] - 216s 5ms/step - loss: 16264533.0000 - val_loss: 15825403.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0498effe50>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Note that it is difficult to use vectors and achieve excellent results due to poor computational power that we have and the ram of google colab was crashing so we convert the problem into a linear problem"
      ],
      "metadata": {
        "id": "RK9TsAi9Ekgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_input():\n",
        "    # Get user input\n",
        "    input_str = input()\n",
        "\n",
        "    # Check if user wants to exit\n",
        "    if input_str == '-1':\n",
        "        return -1, ''\n",
        "\n",
        "    # Tokenize the input sentence\n",
        "    tokenized_word = tokenizer.texts_to_sequences([input_str])\n",
        "\n",
        "    # Pad the tokenized input sequence to have length 1\n",
        "    padded_word = pad_sequences(tokenized_word, maxlen=1)\n",
        "\n",
        "    return padded_word, input_str"
      ],
      "metadata": {
        "id": "sREOqwazpETD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_output(number):\n",
        "    # Pad the number sequence to have length 1\n",
        "    padded_sequence = pad_sequences(number, maxlen=1)\n",
        "\n",
        "    # Convert the padded sequence back to text\n",
        "    text = tokenizer.sequences_to_texts(padded_sequence)\n",
        "\n",
        "    # Return the predicted word as text\n",
        "    return text[0]"
      ],
      "metadata": {
        "id": "VIM_4fUJ4lMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "  first_loop = True\n",
        "  correct = False\n",
        "  sentence = \"\"\n",
        "\n",
        "  while True:\n",
        "      if first_loop:\n",
        "          print(\"Enter Next word (-1 to terminate):\")\n",
        "          first_loop = False\n",
        "      else:\n",
        "          if not correct:\n",
        "              print(\"Sorry, Enter Next word (-1 to terminate):\")\n",
        "\n",
        "      if correct:\n",
        "          # Predict the next word based on the previous prediction\n",
        "          tokenized_word = tokenizer.texts_to_sequences([predicted_word])\n",
        "          predicted_word = pad_sequences(tokenized_word, maxlen=1)\n",
        "          predicted_word = model.predict(predicted_word, verbose=0)\n",
        "          predicted_word = prepare_output(predicted_word)\n",
        "      else:\n",
        "          # Get the input word from the user\n",
        "          word, string = prepare_input()\n",
        "          if word == -1:\n",
        "              print(f\"Your final Sentence is “{sentence}”\")\n",
        "              break\n",
        "          sentence += string + \" \"\n",
        "\n",
        "          # Predict the next word based on the user input\n",
        "          predicted_word = model.predict(word, verbose=0)\n",
        "          predicted_word = prepare_output(predicted_word)\n",
        "\n",
        "      print(f\"Is your next word: “{predicted_word}”? [y] = yes , [n] = no\")\n",
        "      ans = input()\n",
        "      if ans == 'y':\n",
        "          correct = True\n",
        "          sentence += predicted_word + \" \"\n",
        "          continue\n",
        "      else:\n",
        "          correct = False\n",
        "          continue"
      ],
      "metadata": {
        "id": "8okuXHlufARj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test()"
      ],
      "metadata": {
        "id": "0B0xvVFfe_fy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "712ddd58-85ac-4247-92b0-9908f8a6d4d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Next word (-1 to terminate):\n",
            "i\n",
            "Is your next word: “shall”? [y] = yes , [n] = no\n",
            "y\n",
            "Is your next word: “suggested”? [y] = yes , [n] = no\n",
            "n\n",
            "Sorry, Enter Next word (-1 to terminate):\n",
            "go\n",
            "Is your next word: “significant”? [y] = yes , [n] = no\n",
            "n\n",
            "Sorry, Enter Next word (-1 to terminate):\n",
            "to\n",
            "Is your next word: “nice”? [y] = yes , [n] = no\n",
            "n\n",
            "Sorry, Enter Next word (-1 to terminate):\n",
            "the\n",
            "Is your next word: “agent”? [y] = yes , [n] = no\n",
            "y\n",
            "Is your next word: “therefore”? [y] = yes , [n] = no\n",
            "n\n",
            "Sorry, Enter Next word (-1 to terminate):\n",
            "and\n",
            "Is your next word: “hit”? [y] = yes , [n] = no\n",
            "y\n",
            "Is your next word: “hearing”? [y] = yes , [n] = no\n",
            "n\n",
            "Sorry, Enter Next word (-1 to terminate):\n",
            "him\n",
            "Is your next word: “executive”? [y] = yes , [n] = no\n",
            "n\n",
            "Sorry, Enter Next word (-1 to terminate):\n",
            "-1\n",
            "Your final Sentence is “i shall go to the agent and hit him ”\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test()"
      ],
      "metadata": {
        "id": "0GeNQOnee_ZZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb304f7a-6ad5-4289-be5b-b0e7fee85c3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Next word (-1 to terminate):\n",
            "my\n",
            "Is your next word: “supplier”? [y] = yes , [n] = no\n",
            "n\n",
            "Sorry, Enter Next word (-1 to terminate):\n",
            "name\n",
            "Is your next word: “need”? [y] = yes , [n] = no\n",
            "n\n",
            "Sorry, Enter Next word (-1 to terminate):\n",
            "is\n",
            "Is your next word: “owned”? [y] = yes , [n] = no\n",
            "n\n",
            "Sorry, Enter Next word (-1 to terminate):\n",
            "marwan\n",
            "Is your next word: “weekly”? [y] = yes , [n] = no\n",
            "n\n",
            "Sorry, Enter Next word (-1 to terminate):\n",
            "-1\n",
            "Your final Sentence is “my name is marwan ”\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test()"
      ],
      "metadata": {
        "id": "uZXYixqbe_S7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22c91f1b-874b-4c2b-b7d5-8268deebd0cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Next word (-1 to terminate):\n",
            "do\n",
            "Is your next word: “organization”? [y] = yes , [n] = no\n",
            "y\n",
            "Is your next word: “speaker”? [y] = yes , [n] = no\n",
            "y\n",
            "Is your next word: “central”? [y] = yes , [n] = no\n",
            "n\n",
            "Sorry, Enter Next word (-1 to terminate):\n",
            "is\n",
            "Is your next word: “owned”? [y] = yes , [n] = no\n",
            "n\n",
            "Sorry, Enter Next word (-1 to terminate):\n",
            "a\n",
            "Is your next word: “pressure”? [y] = yes , [n] = no\n",
            "n\n",
            "Sorry, Enter Next word (-1 to terminate):\n",
            "loyer\n",
            "Is your next word: “weekly”? [y] = yes , [n] = no\n",
            "n\n",
            "Sorry, Enter Next word (-1 to terminate):\n",
            "-1\n",
            "Your final Sentence is “do organization speaker is a loyer ”\n"
          ]
        }
      ]
    }
  ]
}